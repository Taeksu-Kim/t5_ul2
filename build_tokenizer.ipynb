{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "from transformers import T5TokenizerFast\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, trainers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from mecab import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': {'id': 0, 'token': '[PAD]'},\n",
       " 'BOS': {'id': 1, 'token': '[BOS]'},\n",
       " 'EOS': {'id': 2, 'token': '[EOS]'},\n",
       " 'UNK': {'id': 3, 'token': '[UNK]'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\"]\n",
    "\n",
    "special_token_dict = {}\n",
    "\n",
    "for i in range(len(special_tokens)):\n",
    "    special_token_dict[special_tokens[i][1:-1]] = {\n",
    "        \"id\" : i,\n",
    "        \"token\" : special_tokens[i]\n",
    "    }\n",
    "\n",
    "special_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = special_token_dict\n",
    "special_tokens_list = [None] * len(special_tokens)\n",
    "for token_dict in special_tokens.values():\n",
    "    special_tokens_list[token_dict[\"id\"]] = token_dict[\"token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TemplateProcessing_dict = {\n",
    "    \"single\" : \"$A {}\".format(special_token_dict['EOS']['token']),\n",
    "    \"special_tokens\" : [\n",
    "        (special_token_dict['BOS']['token'], special_token_dict['BOS']['id']),\n",
    "        (special_token_dict['EOS']['token'], special_token_dict['EOS']['id']),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_keys = ['Nmt', 'NFKC', 'Replace', 'Lowercase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizers_dict = {\n",
    "    'Nmt' : normalizers.Nmt(),\n",
    "    'NFKC' : normalizers.NFKC(),\n",
    "    'Replace' : normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    'Lowercase' : normalizers.Lowercase(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [ normalizers_dict[key] for key in norm_keys]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = TemplateProcessing(**TemplateProcessing_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files=[\"namuwiki_mecab.txt\"], vocab_size=32_000, min_frequency=2, special_tokens=special_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ã', 'ģ', 'Ĥ', 'f']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.tokenize(\"あｆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "그 이외에도 맵을 훨씬 화려하게 개편하거나, 주요 npc들의 배틀 장면에 애니메이션을 추가함으로써 훨씬 게임이 다채로워졌다. 또한 컨텐츠도 많이 늘어나서 즐길거리가 엄청나게 많아졌다.\n",
    "\n",
    "특히 메인 스토리의 부실은 DP의 가장 큰 문제점으로 지적받았는데 최종보스 태홍의 비중을 늘리고, 그의 목적과 과거에 대해 훨씬 더 자세히 설명하며, 핸섬, 플루토 등의 주요 캐릭터를 더 추가하여 메인 스토리의 재미를 더 부가했다.\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그 이외에도 맵을 훨씬 화려하게 개편하거나, 주요 npc들의 배틀 장면에 애니메이션을 추가함으로써 훨씬 게임이 다채로워졌다. 또한 컨텐츠도 많이 늘어나서 즐길거리가 엄청나게 많아졌다. 특히 메인 스토리의 부실은 dp의 가장 큰 문제점으로 지적받았는데 최종보스 태홍의 비중을 늘리고, 그의 목적과 과거에 대해 훨씬 더 자세히 설명하며, 핸섬, 플루토 등의 주요 캐릭터를 더 추가하여 메인 스토리의 재미를 더 부가했다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(t5_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30950, 489, 11520, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.encode(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[163,\n",
       " 227,\n",
       " 236,\n",
       " 163,\n",
       " 227,\n",
       " 111,\n",
       " 163,\n",
       " 228,\n",
       " 234,\n",
       " 163,\n",
       " 227,\n",
       " 232,\n",
       " 163,\n",
       " 6034,\n",
       " 163,\n",
       " 227,\n",
       " 248,\n",
       " 163,\n",
       " 227,\n",
       " 230,\n",
       " 163,\n",
       " 227,\n",
       " 126,\n",
       " 163,\n",
       " 227,\n",
       " 251,\n",
       " 4,\n",
       " 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.encode(\"おはようございます！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = t5_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentinel_tokens = 100\n",
    "\n",
    "sentinel_list = []\n",
    "for i in range(num_sentinel_tokens):\n",
    "    sentinel_list.append('[sentinel_{}]'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_list.extend(['[R]','[X]','[S]','[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': sentinel_list}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.bos_token_id = 1\n",
    "tokenizer.eos_token_id = 2\n",
    "tokenizer.unk_token_id = 3\n",
    "tokenizer.sep_token_id = 32103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32104"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_save_path = '../namuwiki_ul2_tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../namuwiki_ul2_tokenizer/tokenizer_config.json',\n",
       " '../namuwiki_ul2_tokenizer/special_tokens_map.json',\n",
       " '../namuwiki_ul2_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer =  tokenizer.from_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32104"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2b8ac7960be3879eb581b5496e0b192cd3e6b9cb327c3e4fb912827016126d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
